{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-1193d038-e89f-46a1-a557-47bc0708a41f","deepnote_cell_type":"code"},"source":"------------------\nMain CMU Lecture\n------------------\n\nBasic Theory; https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c9ca0056-acb6-4c23-bcc9-aaff0120b058\n\n\nNotes: \n\n- They try to balance factors like accuracy, novelty, dispersity and stability in the recommendations\n-  The type of data available in its database (e.g., ratings, user registration information, features and content for items that can be\nranked, social relationships among users and location-aware\ninformation).\n-  The filtering algorithm used (e.g., demographic, content-based, collaborative, social-based, context-aware and hybrid).\n- The model chosen (e.g., based on direct use of data: ‘‘memorybased,’’ or a model generated using such data: ‘‘model-based’’)\n- The employed techniques are also considered: probabilistic approaches, Bayesian networks, nearest neighbors algorithm;\nbio-inspired algorithms such as neural networks and genetic algorithms; fuzzy models, singular value decomposition techniques to reduce sparsity levels, etc.\n- Sparsity level of the database and the desired scalability.\n- Performance of the system (time and memory consuming).\n- The objective sought is considered (e.g., predictions and top N\nrecommendations) as well as\n- The desired quality of the results (e.g., novelty, coverage and\nprecision).\n\n----------------------------------------------------------------------------\n\n- Quality of the predictions: mean absolute error, accuracy and coverage\n\n- Quality of the set of recommendations: precision, recall and F1 \n\n- Quality of the list of recommendations: rank measures (half-life, discounted cumulative gain)\n\n-------------------\nBasic Experiments\n-------------------\n\nBasic Experiment: https://blog.fastforwardlabs.com/2018/01/22/exploring-recommendation-systems.html\n\nAnother python tutorial https://www.youtube.com/watch?v=tooddaC14q4&ab_channel=MadhavThaker\n\nCollaborative Filtering https://youtu.be/A259Yo8hBRs?t=1422\n\nLogistic Matrix Factorization for Implicit Feedback Data: \nhttps://web.stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf\n\nSemantic recommenderations \n\nNote that there exist various open source datasets as well. e.g Million Song Dataset\n\n\n-----------------\nAdvanced \n------------------\n\nSurvey paper: https://romisatriawahono.net/lecture/rm/survey/information%20retrieval/Bobadilla%20-%20Recommender%20Systems%20-%202013.pdf\n\nNotes: \n\n\n\nCollaborative Filtering for Implicit Feedback Datasets\n\n\nLogistic Matrix Factorization for Implicit\nFeedback Data\n\nSequential Recommender Systems: Challenges, Progress and Prospects \n\n\nExplore, Exploit, and Explain: Personalizing Explainable\nRecommendations with Bandits\n\n\n----------------------------------\nideas - Can we build a next gen system using three big ideas: \n----------------------------------\n\n1. learning to ranking; \n\nhttps://medium.com/@nikhilbd/intuitive-explanation-of-learning-to-rank-and-ranknet-lambdarank-and-lambdamart-fe1e17fac418\n\n2. Semantic recommenderations: https://learning.oreilly.com/videos/strata-data-conference/9781492025856/9781492025856-video322864?autoplay=false\n\n3. Using RL for recommendations: see paper : Explore, Exploit, and Explain: Personalizing Explainable\nRecommendations with Bandits\n","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"339a9b92-9134-4785-a586-8127e19f3269","deepnote_execution_queue":[]}}